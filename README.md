# Multi-Agent Conversational Debate System  

## Project Overview

This project implements a **multi-agent debate system** where three distinct personas â€” **Commander**, **Rationalist**, and **Dramatist** â€” engage in structured debates to collaboratively reason about complex queries. Their discussion concludes with a **Synthesizer Agent**, which merges all viewpoints into a single, unified insight.

The goal is to simulate realistic collaborative reasoning between agents with different cognitive biases and conversational styles. Each agent retrieves evidence from its **persona-specific knowledge base**, debates in multiple rounds, and contributes to the final synthesis.

The system is built using:
- **LangGraph** â€” for orchestration  
- **FAISS** â€” for retrieval  
- **Sentence-Transformers (SBERT)** â€” for embeddings  
- **Streamlit** â€” for user interaction  


## Personas & Discovery Process

The three personas were designed from patterns discovered in movie dialogues using data-driven persona discovery.

### Data Source
- **Dataset:** [Cornell Movie-Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)  
- Contains extracted dialogues from films (`data/raw/movie_data`)  
- Each line includes the **character**, **movie**, and **dialogue text**

  
### Persona Discovery Workflow
1. **Preprocessing**
   - Lowercasing, punctuation removal, token filtering  
   - Removed non-dialogue noise and duplicates  
   - Filtered sentences between 5 and 220 characters

2. **Topic Modeling & Clustering**
   - Used **SentenceTransformer (all-MiniLM-L6-v2)** embeddings  
   - Clustered utterances by semantic similarity  
   - Manually labeled clusters based on tone and reasoning patterns

3. **Final Personas**

| Persona | Description | Traits |
|----------|--------------|--------|
| ğŸ§­ **Commander** | Tactical and action-oriented | Decisive, structured, direct |
| ğŸ§  **Rationalist** | Analytical, logic-based thinker | Data-driven, hypothesis-testing |
| ğŸ­ **Dramatist** | Emotionally expressive | Reflective, metaphorical, humanistic |

4. **Synthesizer Agent**
   - Combines all three perspectives  
   - Produces a balanced, human-readable unified insight  

To regenerate persona clusters and `persona_analysis.json`:
```bash
python src/persona_discovery.py
```


## Collaboration Architecture

The system uses **LangGraph** to model agent interactions as a **state machine** of connected nodes.

| Phase | Description | Node Function |
|--------|--------------|----------------|
| **Init** | Initializes agents, state, and dialogue | `init_state_node` |
| **Round 1** | Each persona gives their independent viewpoint | `round1_node` |
| **Dialogue** | Agents converse & reference each other | `dialogue_node` |
| **Challenge** | Cross-critiques and rebuttals | `challenges_node` |
| **Synthesis** | Unified summary of insights | `synthesis_node` |

The `router` function manages transitions between phases based on conversation state.  
`GraphState` tracks queries, dialogue history, and agent responses.


## Rebuilding Data Artifacts (FAISS + Persona Meta)

We intentionally do **not** commit large artifacts. Rebuild locally:

1. Place a dialog dataset under `data/raw/movie_data/` (e.g., *Cornell Movie-Dialogs Corpus*).  
2. Preprocess & discover personas:
   ```bash
   PYTHONPATH=. python src/preprocessing.py
   PYTHONPATH=. python src/persona_discovery.py
   ```
3. Build persona indices:
   ```bash
   PYTHONPATH=. python src/build_indices.py
   ```

This generates:
```
data/processed/personas/<agent>/
  â”œâ”€ <agent>.meta.jsonl
  â””â”€ <agent>.faiss
```


## âš™ï¸ Setup & Running Instructions

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/eashita11/vectorial-multiagent.git
cd vectorial-multiagent
```

### 2ï¸âƒ£ Create Virtual Environment
```bash
python -m venv .venv
source .venv/bin/activate   # macOS/Linux
# or
.venv\Scripts\activate      # Windows
```

### 3ï¸âƒ£ Install Requirements
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Start Ollama & Run App
Start the Ollama Llama3 server locally:
```bash
brew install ollama          # macOS
ollama serve
ollama pull llama3
```

Then launch Streamlit:
```bash
streamlit run app.py
```


## Tests

All test files are located in `/tests/`.

**Quick subset:**
```bash
PYTHONPATH=. pytest -q -k "retriever or citations"
```

**Full suite (can be slow on CPU):**
```bash
PYTHONPATH=. pytest -q --maxfail=1
```

> **Note:** FAISS and model initialization can slow tests down.


## Dataset & Preprocessing

Processed dataset files are large, so they are not included in the repository.  
To recreate them:

```bash
python src/preprocessing.py
python src/persona_discovery.py
```

You may alternatively use an open-source dataset such as the *Cornell Movie Dialogs Corpus*.


## Usage Guide

1. Launch the Streamlit app.  
2. Ask any question (e.g., â€œHow can I rebuild trust in a failing team?â€).  
3. Observe:
   - **Round 1:** Individual viewpoints  
   - **Dialogue:** Agents build on each other  
   - **Challenges:** Cross-critiques  
   - **Synthesis:** Unified insight  

### Showcase Prompts
- â€œWe missed our Q3 retention target; CS says itâ€™s product bugs, PM says onboarding. What should we do this sprint to stabilize churn?â€  
- â€œMarketing wants to launch in 4 weeks; infra warns cost blow-up at 3Ã— traffic. How do we decide, and whatâ€™s the minimum we ship safely?â€


## Citation & Attribution System

Each agent uses a **retriever** backed by **FAISS** and **SBERT embeddings**:
- Each persona retrieves snippets aligned with its reasoning style.
- Citations appear in the UI (e.g., `ğŸ”— HAIG Â· line L421488 Â· movie m150`).
- The **Synthesizer Agent** only reuses citations provided by personas â€” it never invents new ones.


## User Interface

A **Streamlit app (`app.py`)** provides a chat-based interface:

- Type your question into the input box.  
- Watch color-coded persona responses:
  - ğŸ§­ **Commander** â†’ Blue border  
  - ğŸ§  **Rationalist** â†’ Green border  
  - ğŸ­ **Dramatist** â†’ Purple border  
  - ğŸ§© **Synthesis** â†’ Teal border  
- Smooth typing animation simulates a live debate.


## Repository Structure
```
â”œâ”€â”€ app.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ persona_analysis.json
â”‚       â””â”€â”€ personas/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ graph/
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ persona_discovery.py
â”‚   â”œâ”€â”€ retriever.py
â”‚   â”œâ”€â”€ orchestrator.py
â”‚   â””â”€â”€ llm.py
â””â”€â”€ tests/
```


## Limitations & Future Work

| Area | Current Limitation | Future Improvement |
|------|--------------------|--------------------|
| Dialogue Depth | Some agents dominate or repeat | Add memory balancing & attention control |
| Persona Contrast | Commander and Rationalist occasionally overlap | Refine prompts for stronger stylistic contrast |
| Speed | Local LLM inference is slow | Enable async calls or caching |
| Retrieval | Static FAISS indices | Add dynamic persona-specific retraining |


## Demo

Below is a glimpse of the Multi-Agent Debate in action:

| Stage | Screenshot |
|--------|-------------|
| ğŸ—£ï¸ Round 1 â€” Individual Perspectives | ![Round1](assets/round1.png) |
| ğŸ’¬ Dialogue â€” Agents Build on Each Other | ![Dialogue](assets/dialogue.png) |
| âš–ï¸ Challenge Round â€” Rebuttals | ![Challenge](assets/challenge.png) |
| ğŸ§© Consensus Summary â€” Unified Insight | ![Synthesis](assets/synthesis.png) |

### Typing Animation Preview
![Typing Demo](assets/typing.gif)


## Credits

Developed by **Eashita Dhillon**  
